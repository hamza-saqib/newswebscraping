{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "from datetime import date\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['title','category', 'chanel', 'permalink', 'image_src', 'date_time', 'description', 'fetching_date', 'language']\n",
    "rows, cols = (0, 9)\n",
    "data = [[0]*cols]*rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [[\"https://www.dawn.com/trends/coronavirus\", \"Health\", \"Dawn\"],\n",
    "           [\"https://www.dawn.com/sport\", \"Sports\", \"Dawn\"], \n",
    "           [\"https://www.dawn.com/world\", \"International\", \"Dawn\"],\n",
    "           [\"https://images.dawn.com/art-culture\", \"Culture\", \"Dawn\"],\n",
    "           [\"https://www.dawn.com/trends/coronavirus\", \"IT/Science\", \"Dawn\"]]\n",
    "#sourceHtml = requests.get(sources[1][0]).text\n",
    "#soup = BeautifulSoup(sourceHtml, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for source in sources:\n",
    "    #print('i' + str(i))\n",
    "    j=0\n",
    "    \n",
    "    sourceHtml = requests.get(source[0]).text\n",
    "    soup = BeautifulSoup(sourceHtml, 'lxml')\n",
    "    for article in soup.find_all('article', class_='box'):\n",
    "        #print('j' + str(j))\n",
    "        j = j +1\n",
    "        i=i+1\n",
    "        try:\n",
    "            title = article.h2.a.text.splitlines()[-1]\n",
    "            try:\n",
    "                permalink = article.find('a', class_='story__link')['href']\n",
    "            except:\n",
    "                permalink = ''\n",
    "            try:\n",
    "                image_src = article.figure.div.a.picture.img['src']\n",
    "            except:\n",
    "                image_src = ''\n",
    "            try:\n",
    "                date_time = article.find('span', class_='timestamp--time').text\n",
    "            except:\n",
    "                date_time = ''\n",
    "            try:\n",
    "                description = article.find('div', class_='story__excerpt').text.splitlines()[0]\n",
    "            except:\n",
    "                description = ''\n",
    "            data.append([title, source[1], source[2], permalink, image_src, date_time, description, date.today(), 'English'])\n",
    "        except:\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources2 = [[\"https://arynews.tv/en/category/international-2\", \"International\", \"ARY\"],\n",
    "            [\"https://arynews.tv/en/category/sci-techno/\", \"IT/Science\", \"ARY\"], \n",
    "            [\"https://arynews.tv/en/category/health-2/\", \"Health\", \"ARY\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "#sourceHtml = requests.get(sources2[1][0], headers=headers).text\n",
    "#soup2 = BeautifulSoup(sourceHtml, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for source in sources2:\n",
    "    #print('i' + str(i))\n",
    "    j=0\n",
    "    \n",
    "    sourceHtml = requests.get(source[0], headers=headers).text\n",
    "    soup = BeautifulSoup(sourceHtml, 'lxml')\n",
    "    for article2 in soup.find_all('article', class_='type-post'):\n",
    "        #print('j' + str(j))\n",
    "        j = j +1\n",
    "        i=i+1\n",
    "        try:\n",
    "            title = article2.div.h2.text\n",
    "        except:\n",
    "            title = ''\n",
    "        try:\n",
    "            permalink = article2.div.find('div', class_='featured').a['href']\n",
    "            \n",
    "        except:\n",
    "            permalink = ''\n",
    "        try:\n",
    "            image_src = article2.div.find('div', class_='featured').a['data-src']\n",
    "        except:\n",
    "            image_src = ''\n",
    "        try:\n",
    "            date_time = article2.div.text.splitlines()[-2]\n",
    "        except:\n",
    "            date_time = ''\n",
    "        try:\n",
    "            description = article2.div.find('div', class_='post-summary').text\n",
    "        except:\n",
    "            description = ''\n",
    "        data.append([title, source[1], source[2], permalink, image_src, date_time, description, date.today(), 'English'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources3 = [[\"https://www.geo.tv/category/world\", \"International\", \"GEO\"],\n",
    "            [\"https://www.geo.tv/category/sci-tech\", \"IT/Science\", \"GEO\"], \n",
    "            [\"https://www.geo.tv/category/sports\", \"Sports\", \"GEO\"],\n",
    "            [\"https://www.geo.tv/category/entertainment\", \"Entertainment\", \"GEO\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceHtml = requests.get(sources3[0][0]).text\n",
    "soup = BeautifulSoup(sourceHtml, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for source in sources3:\n",
    "    #print('i' + str(i))\n",
    "    j=0\n",
    "    \n",
    "    sourceHtml = requests.get(source[0]).text\n",
    "    soup = BeautifulSoup(sourceHtml, 'lxml')\n",
    "    for article in soup.find_all('div', class_='singleBlock'):\n",
    "        #print('j' + str(j))\n",
    "        j = j +1\n",
    "        i=i+1\n",
    "        try:\n",
    "            title = article.div.ul.li.a.find('div', class_='entry-content-heading').div.h2.text\n",
    "        except:\n",
    "            title = ''\n",
    "        try:\n",
    "            permalink = article.div.ul.li.a['href']\n",
    "        except:\n",
    "            permalink = ''\n",
    "            print('ex---')\n",
    "        try:\n",
    "            image_src = article.div.ul.li.a.find('div', class_='pic').img['src']\n",
    "        except:\n",
    "            image_src = ''\n",
    "        try:\n",
    "            date_time = article.div.ul.li.a.find('div', class_='entry-content-heading').div.span.text\n",
    "        except:\n",
    "            date_time = ''\n",
    "        try:\n",
    "            description = ''\n",
    "        except:\n",
    "            description = ''\n",
    "        data.append([title, source[1], source[2], permalink, image_src, date_time, description, date.today(), 'English'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources4 = [[\"https://ptv.com.pk/ptvWorld/news/National\", \"National/Local\", \"PTV\"],\n",
    "            [\"https://ptv.com.pk/ptvWorld/news/Cricket\", \"Sports\", \"PTV\"], \n",
    "            [\"https://ptv.com.pk/ptvWorld/news/International\", \"International\", \"PTV\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for source in sources4:\n",
    "    #print('i' + str(i))\n",
    "    j=0\n",
    "    \n",
    "    sourceHtml = requests.get(source[0]).text\n",
    "    soup = BeautifulSoup(sourceHtml, 'lxml')\n",
    "    for article in soup.find_all('ul', class_='big'):\n",
    "        #print('j' + str(j))\n",
    "        j = j +1\n",
    "        i=i+1\n",
    "        try:\n",
    "            title = article.li.div.div.h4.a.text.splitlines()[-1]\n",
    "        except:\n",
    "            title = ''\n",
    "        try:\n",
    "            permalink = article.li.div.div.h4.a['href']\n",
    "            \n",
    "        except:\n",
    "            permalink = ''\n",
    "        try:\n",
    "            image_src = article.li.img['src']\n",
    "        except:\n",
    "            image_src = ''\n",
    "        try:\n",
    "            date_time = article.li.div.div.ul.li.text.splitlines()[-1]\n",
    "        except:\n",
    "            date_time = ''\n",
    "        try:\n",
    "            description = article.li.div.div.p.text.splitlines()[-1]\n",
    "        except:\n",
    "            description = ''\n",
    "        data.append([title, source[1], source[2], permalink, image_src, date_time, description, date.today(), 'English'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources5 = [[\"https://www.express.pk/health/\", \"Health\", \"Express\"],\n",
    "            [\"https://www.express.pk/sports/\", \"Sports\", \"Express\"], \n",
    "            [\"https://www.express.pk/world/\", \"International\", \"Express\"],\n",
    "            [\"https://www.express.pk/science/\", \"IT/Science\", \"Express\"],\n",
    "            [\"https://www.express.pk/pakistan/\", \"National/Local\", \"Express\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for source in sources5:\n",
    "    #print('i' + str(i))\n",
    "    j=0\n",
    "    \n",
    "    sourceHtml = requests.get(source[0]).text\n",
    "    soup = BeautifulSoup(sourceHtml, 'lxml')\n",
    "    for article in soup.find_all('div', class_='cstoreyitem'):\n",
    "        #print('j' + str(j))\n",
    "        j = j +1\n",
    "        i=i+1\n",
    "        try:\n",
    "            title = article.div.a.find_next_sibling(\"a\").text\n",
    "        except:\n",
    "            title = ''\n",
    "        try:\n",
    "            permalink = article.div.a['href']\n",
    "            \n",
    "        except:\n",
    "            permalink = ''\n",
    "        try:\n",
    "            image_src = article.div.a.img['src']\n",
    "        except:\n",
    "            image_src = ''\n",
    "        try:\n",
    "            date_time = ''\n",
    "        except:\n",
    "            date_time = ''\n",
    "        try:\n",
    "            description = article.div.a.img['alt']\n",
    "        except:\n",
    "            description = ''\n",
    "        data.append([title, source[1], source[2], permalink, image_src, date_time, description, date.today(), 'Urdu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources6 = [[\"https://www.samaa.tv/lifeandstyle/\", \"LifeStyle/Culture\", \"SAMAA\"],\n",
    "            [\"https://www.samaa.tv/sports/\", \"Sports\", \"SAMAA\"], \n",
    "            [\"https://www.samaa.tv/news/\", \"Local\", \"SAMAA\"],\n",
    "            [\"https://www.samaa.tv/technology/\", \"IT/Science\", \"SAMAA\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these webpages have different style of posts on top\n",
    "i = 0\n",
    "for source in sources6:\n",
    "    #print('i' + str(i))\n",
    "    j=0\n",
    "    \n",
    "    sourceHtml = requests.get(source[0]).text\n",
    "    soup = BeautifulSoup(sourceHtml, 'lxml')\n",
    "    \n",
    "    first_article = soup.find_all('div', class_='col-md-12')[2].find('div', class_='row')\n",
    "    try:\n",
    "        title = first_article.a['title']\n",
    "    except:\n",
    "        title = ''\n",
    "    try:\n",
    "        permalink = first_article.a['href']\n",
    "\n",
    "    except:\n",
    "        permalink = ''\n",
    "    try:\n",
    "        image_src = first_article.a.img['src']\n",
    "    except:\n",
    "        image_src = ''\n",
    "    \n",
    "    date_time = ''\n",
    "    description = ''\n",
    "    data.append([title, source[1], source[2], permalink, image_src, date_time, description, date.today(), 'English'])\n",
    "    \n",
    "    for article in soup.find_all('div', class_='categorycolumn'):\n",
    "        #print('j' + str(j))\n",
    "        j = j +1\n",
    "        if j==1:\n",
    "            \n",
    "            try:\n",
    "                title = article.div.div.a['title']\n",
    "            except:\n",
    "                title = ''\n",
    "            try:\n",
    "                permalink = article.div.div.a['href']\n",
    "\n",
    "            except:\n",
    "                permalink = ''\n",
    "            try:\n",
    "                image_src = article.div.div.a.img['src']\n",
    "            except:\n",
    "                image_src = ''\n",
    "                \n",
    "            date_time = ''\n",
    "            description = ''\n",
    "            data.append([title, source[1], source[2], permalink, image_src, date_time, description, date.today(), 'English'])\n",
    "        elif j>1:\n",
    "            article = article.find('div', class_='row').find('div', class_='respnb1')\n",
    "            try:\n",
    "                title = article.a['title']\n",
    "            except:\n",
    "                title = ''\n",
    "            try:\n",
    "                permalink = article.a['href']\n",
    "\n",
    "            except:\n",
    "                permalink = ''\n",
    "            try:\n",
    "                image_src = article.a.img['src']\n",
    "            except:\n",
    "                image_src = ''\n",
    "            date_time = ''\n",
    "            description = ''\n",
    "            data.append([title, source[1], source[2], permalink, image_src, date_time, description, date.today(), 'English'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources7 = [[\"https://dunyanews.tv/en/World\", \"International\", \"Dunya\"],\n",
    "            [\"https://dunyanews.tv/en/Cricket\", \"Sports\", \"Dunya\"], \n",
    "            [\"https://dunyanews.tv/en/Pakistan\", \"Local\", \"Dunya\"],\n",
    "            [\"https://dunyanews.tv/en/Technology\", \"IT/Science\", \"Dunya\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these webpages have different style of posts on top\n",
    "i = 0\n",
    "for source in sources7:\n",
    "    #print('i' + str(i))\n",
    "    j=0\n",
    "    \n",
    "    sourceHtml = requests.get(source[0]).text\n",
    "    soup = BeautifulSoup(sourceHtml, 'lxml')\n",
    "    \n",
    "    for article in soup.find_all('article'):\n",
    "        #print('j' + str(j))\n",
    "        j = j +1\n",
    "        if j<9:\n",
    "            try:\n",
    "                title = article.h3.text\n",
    "                print(title)\n",
    "            except:\n",
    "                title = ''\n",
    "            try:\n",
    "                permalink = article.div.a['href']\n",
    "                print(permalink)\n",
    "            except:\n",
    "                permalink = ''\n",
    "                print(permalink)\n",
    "            try:\n",
    "                image_src = article.div.a.img['src']\n",
    "                print(image_src)\n",
    "            except:\n",
    "                image_src = ''\n",
    "                print(image_src)\n",
    "            try:\n",
    "                date_time = article.find('div', class_='post_text_wrapper').ul.li.a.text\n",
    "                print(date_time)\n",
    "            except:\n",
    "                date_time = ''\n",
    "                print(date_time)\n",
    "            try:\n",
    "                description = article.find('div', class_='text').text\n",
    "                print(description)\n",
    "            except:\n",
    "                description = ''\n",
    "                print(description)\n",
    "            \n",
    "            data.append([title, source[1], source[2], 'https://dunyanews.tv'+permalink, image_src, date_time, description, date.today(), 'English'])\n",
    "        else:\n",
    "            print('-------sss')\n",
    "            try:\n",
    "                title = article.h3.text\n",
    "                print(title)\n",
    "            except:\n",
    "                title = ''\n",
    "            try:\n",
    "                permalink = article.h3.a['href']\n",
    "                print(permalink)\n",
    "            except:\n",
    "                permalink = ''\n",
    "                print(permalink)\n",
    "            try:\n",
    "                image_src = article.div.a.img['src']\n",
    "                print(image_src)\n",
    "            except:\n",
    "                image_src = ''\n",
    "                print(image_src)\n",
    "            try:\n",
    "                date_time = article.ul.li.a.text\n",
    "                print(date_time)\n",
    "            except:\n",
    "                date_time = ''\n",
    "                print(date_time)\n",
    "            try:\n",
    "                description = article.find('div', class_='text').text\n",
    "                print(description)\n",
    "            except:\n",
    "                description = ''\n",
    "                print(description)\n",
    "            \n",
    "            data.append([title, source[1], source[2], 'https://dunyanews.tv'+permalink, image_src, date_time, description, date.today(), 'English'])\n",
    "            print('-------------------------------')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['title','category', 'chanel', 'permalink', 'image_src', 'date_time', 'description', 'fetching_date', 'Language'])\n",
    "df\n",
    "df.to_csv('5webs data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
